{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': True, 'cat': True, 'is': True, 'very': True, 'cute': True}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in nltk.word_tokenize(sent)})\n",
    "\n",
    "print(format_sentence(\"The cat is very cute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 8000 instances, test on 2000 instances\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "with open(\"./data/pos_reviews_5k.txt\") as f:\n",
    "    for i in f:\n",
    "        pos.append([format_sentence(i), 'pos'])\n",
    "        \n",
    "neg = []\n",
    "with open(\"./data/neg_reviews_5k.txt\") as f:\n",
    "    for i in f:\n",
    "        neg.append([format_sentence(i), 'neg'])\n",
    "        \n",
    "training_set_size = .8\n",
    "pos_training_cutoff = int((training_set_size)*len(pos))\n",
    "neg_training_cutoff = int((training_set_size)*len(neg))\n",
    "        \n",
    "training_set = pos[:pos_training_cutoff] + neg[:neg_training_cutoff]\n",
    "test_set = pos[pos_training_cutoff:] + neg[neg_training_cutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(training_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               redeeming = True              neg : pos    =     24.3 : 1.0\n",
      "                   Avoid = True              neg : pos    =     23.0 : 1.0\n",
      "                   Payne = True              neg : pos    =     22.3 : 1.0\n",
      "                   WORST = True              neg : pos    =     22.3 : 1.0\n",
      "                    GTA3 = True              neg : pos    =     21.0 : 1.0\n",
      "                  Batman = True              neg : pos    =     21.0 : 1.0\n",
      "                    Zerg = True              pos : neg    =     19.0 : 1.0\n",
      "               atrocious = True              neg : pos    =     18.3 : 1.0\n",
      "                   lousy = True              neg : pos    =     17.8 : 1.0\n",
      "                Splinter = True              neg : pos    =     16.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training_set)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "neg\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "testPos = \"Dogs are awesome!\"\n",
    "testNeg = \"This game is trash\"\n",
    "testDoubleNeg = \"not bad\"\n",
    "\n",
    "print(classifier.classify(format_sentence(testPos)))\n",
    "print(classifier.classify(format_sentence(testNeg)))\n",
    "print(classifier.classify(format_sentence(testDoubleNeg)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision: 0.9670658682634731\n",
      "Positive recall: 0.646\n",
      "Positive F-Measure: 0.7745803357314149\n",
      "Negative precision: 0.7342342342342343\n",
      "Negative recall: 0.978\n",
      "Negative F-Measure: 0.8387650085763293\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from nltk.metrics.scores import (precision, recall, f_measure)\n",
    "\n",
    "ref_sets = collections.defaultdict(set)\n",
    "test_sets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    ref_sets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    test_sets[observed].add(i)\n",
    "    \n",
    "print('Positive precision: ' + str(precision(ref_sets['pos'], test_sets['pos'])))\n",
    "print('Positive recall: ' + str(recall(ref_sets['pos'], test_sets['pos'])))\n",
    "print('Positive F-Measure: ' + str(f_measure(ref_sets['pos'], test_sets['pos'])))\n",
    "\n",
    "print('Negative precision: ' + str(precision(ref_sets['neg'], test_sets['neg'])))\n",
    "print('Negative recall: ' + str(recall(ref_sets['neg'], test_sets['neg'])))\n",
    "print('Negative F-Measure: ' + str(f_measure(ref_sets['neg'], test_sets['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 6000 instances, test on 4000 instances\n"
     ]
    }
   ],
   "source": [
    "# Training on a medium-sized set\n",
    "        \n",
    "training_set_size2 = .6\n",
    "pos_training_cutoff2 = int((training_set_size2)*len(pos))\n",
    "neg_training_cutoff2 = int((training_set_size2)*len(neg))\n",
    "        \n",
    "training_set2 = pos[:pos_training_cutoff2] + neg[:neg_training_cutoff2]\n",
    "test_set2 = pos[pos_training_cutoff2:] + neg[neg_training_cutoff2:]\n",
    "print('train on %d instances, test on %d instances' % (len(training_set2), len(test_set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier2 = NaiveBayesClassifier.train(training_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision: 0.9434250764525994\n",
      "Positive recall: 0.617\n",
      "Positive F-Measure: 0.7460701330108828\n",
      "Negative precision: 0.7154531946508172\n",
      "Negative recall: 0.963\n",
      "Negative F-Measure: 0.8209718670076727\n"
     ]
    }
   ],
   "source": [
    "ref_sets2 = collections.defaultdict(set)\n",
    "test_sets2 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set2):\n",
    "    ref_sets2[label].add(i)\n",
    "    observed2 = classifier2.classify(feats)\n",
    "    test_sets2[observed2].add(i)\n",
    "    \n",
    "print('Positive precision: ' + str(precision(ref_sets2['pos'], test_sets2['pos'])))\n",
    "print('Positive recall: ' + str(recall(ref_sets2['pos'], test_sets2['pos'])))\n",
    "print('Positive F-Measure: ' + str(f_measure(ref_sets2['pos'], test_sets2['pos'])))\n",
    "\n",
    "print('Negative precision: ' + str(precision(ref_sets2['neg'], test_sets2['neg'])))\n",
    "print('Negative recall: ' + str(recall(ref_sets2['neg'], test_sets2['neg'])))\n",
    "print('Negative F-Measure: ' + str(f_measure(ref_sets2['neg'], test_sets2['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 4000 instances, test on 6000 instances\n"
     ]
    }
   ],
   "source": [
    "# Training on a small set\n",
    "        \n",
    "training_set_size3 = .4\n",
    "pos_training_cutoff3 = int((training_set_size3)*len(pos))\n",
    "neg_training_cutoff3 = int((training_set_size3)*len(neg))\n",
    "        \n",
    "training_set3 = pos[:pos_training_cutoff3] + neg[:neg_training_cutoff3]\n",
    "test_set3 = pos[pos_training_cutoff3:] + neg[neg_training_cutoff3:]\n",
    "print('train on %d instances, test on %d instances' % (len(training_set3), len(test_set3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier3 = NaiveBayesClassifier.train(training_set3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision: 0.9548458149779736\n",
      "Positive recall: 0.289\n",
      "Positive F-Measure: 0.4437052200614125\n",
      "Negative precision: 0.5811076197957581\n",
      "Negative recall: 0.9863333333333333\n",
      "Negative F-Measure: 0.731339594661394\n"
     ]
    }
   ],
   "source": [
    "ref_sets3 = collections.defaultdict(set)\n",
    "test_sets3 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set3):\n",
    "    ref_sets3[label].add(i)\n",
    "    observed3 = classifier3.classify(feats)\n",
    "    test_sets3[observed3].add(i)\n",
    "    \n",
    "print('Positive precision: ' + str(precision(ref_sets3['pos'], test_sets3['pos'])))\n",
    "print('Positive recall: ' + str(recall(ref_sets3['pos'], test_sets3['pos'])))\n",
    "print('Positive F-Measure: ' + str(f_measure(ref_sets3['pos'], test_sets3['pos'])))\n",
    "\n",
    "print('Negative precision: ' + str(precision(ref_sets3['neg'], test_sets3['neg'])))\n",
    "print('Negative recall: ' + str(recall(ref_sets3['neg'], test_sets3['neg'])))\n",
    "print('Negative F-Measure: ' + str(f_measure(ref_sets3['neg'], test_sets3['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 9000 instances, test on 1000 instances\n"
     ]
    }
   ],
   "source": [
    "# Training on a very large set\n",
    "        \n",
    "training_set_size4 = .9\n",
    "pos_training_cutoff4 = int((training_set_size4)*len(pos))\n",
    "neg_training_cutoff4 = int((training_set_size4)*len(neg))\n",
    "        \n",
    "training_set4 = pos[:pos_training_cutoff4] + neg[:neg_training_cutoff4]\n",
    "test_set4 = pos[pos_training_cutoff4:] + neg[neg_training_cutoff4:]\n",
    "print('train on %d instances, test on %d instances' % (len(training_set4), len(test_set4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier4 = NaiveBayesClassifier.train(training_set4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision: 0.9708454810495627\n",
      "Positive recall: 0.666\n",
      "Positive F-Measure: 0.7900355871886121\n",
      "Negative precision: 0.08032207384131972\n",
      "Negative recall: 0.98\n",
      "Negative F-Measure: 0.8470181503889369\n"
     ]
    }
   ],
   "source": [
    "ref_sets4 = collections.defaultdict(set)\n",
    "test_sets4 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set4):\n",
    "    ref_sets4[label].add(i)\n",
    "    observed4 = classifier4.classify(feats)\n",
    "    test_sets4[observed4].add(i)\n",
    "    \n",
    "print('Positive precision: ' + str(precision(ref_sets4['pos'], test_sets4['pos'])))\n",
    "print('Positive recall: ' + str(recall(ref_sets4['pos'], test_sets4['pos'])))\n",
    "print('Positive F-Measure: ' + str(f_measure(ref_sets4['pos'], test_sets4['pos'])))\n",
    "\n",
    "print('Negative precision: ' + str(precision(ref_sets4['neg'], test_sets3['neg'])))\n",
    "print('Negative recall: ' + str(recall(ref_sets4['neg'], test_sets4['neg'])))\n",
    "print('Negative F-Measure: ' + str(f_measure(ref_sets4['neg'], test_sets4['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
